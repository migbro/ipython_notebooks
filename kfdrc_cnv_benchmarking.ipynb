{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sevenbridges as sbg\n",
    "from sevenbridges.errors import SbgError\n",
    "from sevenbridges.http.error_handlers import rate_limit_sleeper, maintenance_sleeper\n",
    "import sys\n",
    "import re\n",
    "import concurrent.futures\n",
    "import pdb\n",
    "config = sbg.Config(profile='turbo')\n",
    "api = sbg.Api(config=config, error_handlers=[rate_limit_sleeper, maintenance_sleeper])\n",
    "project = 'brownm28/mb-controlfreec-troubleshoot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found valid file ccc925f9-29ad-4f16-baec-fc955fe37782.CNVs.p.value.txt PT_9QQ37AWW Tumor\n",
      "Copied file over\n",
      "Found valid file e322d7c1-1654-481f-86ed-2d2f2c4518ed.CNVs.p.value.txt PT_1EQHANKW Tumor\n",
      "Copied file over\n",
      "Found valid file 6bb8fb90-4430-436a-8a8c-2a119f970e69.CNVs.p.value.txt PT_TAJJ9MYY Tumor\n",
      "Copied file over\n",
      "Found valid file 151fe94b-d483-41e3-aa0b-c98f52b193fb.CNVs.p.value.txt PT_BXYKW39H Tumor\n",
      "Copied file over\n",
      "Found valid file 5e1c7ea8-7032-4f37-80fe-04ce13d9cf4e.CNVs.p.value.txt PT_53M7K3JE Tumor\n",
      "Copied file over\n",
      "Found valid file a36f0080-c6a8-4dc8-bbce-f304802a6fad.CNVs.p.value.txt PT_1YAJEAMJ Tumor\n",
      "Copied file over\n",
      "Found valid file 60a9dcdc-586c-4281-be37-fbf81c317742.CNVs.p.value.txt PT_R2TRGY6N Tumor\n",
      "Copied file over\n",
      "Found valid file b8defbad-85da-4746-9b98-300ecbec8e41.CNVs.p.value.txt PT_KWRFGRER Tumor\n",
      "Copied file over\n",
      "Found valid file ec2b7437-801e-4946-9138-3948736c30f6.CNVs.p.value.txt PT_ASH4P45D Tumor\n",
      "Copied file over\n",
      "Found valid file bd65eb61-19e3-4b80-804d-7c51a5ecc7f0.CNVs.p.value.txt PT_3WF5J3PZ Tumor\n",
      "Copied file over\n",
      "Found valid file 3b254807-0fdd-4933-a263-a9e52a2d1b81.CNVs.p.value.txt PT_JF62ZBX8 Tumor\n",
      "Copied file over\n",
      "Found valid file 1d1f661d-5c7a-4c83-a53a-b7c744c5a578.CNVs.p.value.txt PT_GERVNCVY Tumor\n",
      "Copied file over\n"
     ]
    }
   ],
   "source": [
    "germ_origin = 'kfdrc-harmonization/sd-dypmehhf'\n",
    "tumor_origin= 'kfdrc-harmonization/sd-dypmehhf-03/'\n",
    "somatic_origin= 'kfdrc-harmonization/sd-dypmehhf-05'\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "for case_id in case_list:\n",
    "    files = api.files.query(project=somatic_origin, metadata = {'case_id': case_id} )\n",
    "    for file_obj in files:\n",
    "        if re.search(\"p.value.txt$\", file_obj.name):\n",
    "            print(\"Found valid file \" + file_obj.name + \" \" + file_obj.metadata[\"Kids First Participant ID\"] + \" \" + file_obj.metadata[\"sample_type\"])\n",
    "            file_obj.copy(project=project)\n",
    "            print (\"Copied file over\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HC Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['axiomPoly_resource_vcf'] = api.files.query(project=project, names=['Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz'])[0]\n",
    "    ref_dict['reference_dict'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dict'])[0]\n",
    "    ref_dict['dbsnp_vcf'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dbsnp138.vcf'])[0]\n",
    "    ref_dict['hapmap_resource_vcf'] = api.files.query(project=project, names=['hapmap_3.3.hg38.vcf.gz'])[0]\n",
    "    ref_dict['mills_resource_vcf'] = api.files.query(project=project, names=['Mills_and_1000G_gold_standard.indels.hg38.vcf.gz'])[0]\n",
    "    ref_dict['omni_resource_vcf'] = api.files.query(project=project, names=['1000G_omni2.5.hg38.vcf.gz'])[0]\n",
    "    ref_dict['one_thousand_genomes_resource_vcf'] = api.files.query(project=project, names=['1000G_phase1.snps.high_confidence.hg38.vcf.gz'])[0]\n",
    "    #ref_dict['ref_fasta'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['reference_fasta'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    #ref_dict['ref_tar_gz'] = api.files.query(project=project, names=['hg38_snpeff.tgz'])[0]\n",
    "    ref_dict['unpadded_intervals_file'] = api.files.query(project=project, names=['hg38.even.handcurated.20k.intervals'])[0]\n",
    "    ref_dict['wgs_evaluation_interval_list'] = api.files.query(project=project, names=['wgs_evaluation_regions.hg38.interval_list'])[0]\n",
    "    ref_dict['snp_sites'] = api.files.query(project=project, names=['1000G_phase3_v4_20130502.sites.hg38.vcf'])[0]\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_gvcf_test-manifest.csv')\n",
    "head = next(manifest)\n",
    "ref_obj = get_refs(api)\n",
    "# app_name = project + \"/kf-single-genotype/0\"\n",
    "app_name = project + \"/kfdrc-single-genotype-basic\"\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_dict['input_vcfs'] = [api.files.get(info[0])]\n",
    "    task_name = \"SINGLE GENOTYPE GATK: \" + info[6] + \" \" + info[11] + \" \" + info[-2]\n",
    "    task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, execution_settings = {'use_memoization': True}, run=False)\n",
    "    task.inputs['output_basename'] = task.id\n",
    "    \n",
    "    task.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expand view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ControlFreeC Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['chr_len'] = api.files.query(project=project, names=['hs38_chr.len'])[0]\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['reference_fai'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta.fai'])[0]\n",
    "    ref_dict['include_expression'] = 'FILTER=\"PASS\"'\n",
    "    ref_dict['coeff_var'] = 0.05\n",
    "    ref_dict['mate_orientation_control'] = \"FR\"\n",
    "    ref_dict['mate_orientation_sample'] = \"FR\"\n",
    "    ref_dict['ploidy'] = [2,3,4]\n",
    "    ref_dict['threads'] = 16\n",
    "    ref_dict['contamination_adjustment'] = True\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Project Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = project + '/kfdrc-controlfreec-wf'\n",
    "manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_crams-manifest.csv')\n",
    "# case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/missed_case_list.txt')\n",
    "sex_prediction = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_sex_info_w_case_id.txt')\n",
    "# b_allele = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_ballele-manifest.csv')\n",
    "b_allele = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/missed_vcf-manifest.csv')\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "ref_objs = get_cf_refs(api)\n",
    "head = next(manifest)\n",
    "\n",
    "bam_dict = {}\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    stype = info[9]\n",
    "    sample_id = info[11]\n",
    "    fid = info[0]\n",
    "    if case_id not in bam_dict:\n",
    "        bam_dict[case_id] = {}\n",
    "    bam_dict[case_id][stype] = {}\n",
    "    bam_dict[case_id][stype]['sid'] = sample_id\n",
    "    bam_dict[case_id][stype]['file_obj'] = api.files.get(fid)\n",
    "    \n",
    "sex_dict = {}\n",
    "s_trans = {\"Male\": \"XY\", \"Female\": \"XX\"}\n",
    "head = next(sex_prediction)\n",
    "for line in sex_prediction:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    case_id = info[4]\n",
    "    pred_sex = info[-1]\n",
    "    ds_sex = info[1]\n",
    "    if case_id in case_list:\n",
    "        if pred_sex != \"Unknown\" and pred_sex == ds_sex:\n",
    "            sex_dict[case_id] = s_trans[pred_sex]\n",
    "        else:\n",
    "            sys.stderr.write(\"Warn, prediction for \" + case_id + \" was inconclusive.  Default to reported sex\\n\")\n",
    "            sex_dict[case_id] = s_trans[ds_sex]\n",
    "b_dict = {}\n",
    "head = next(b_allele)\n",
    "for line in b_allele:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    b_dict[case_id] = api.files.get(info[0])\n",
    "for case_id in case_list:\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        for key in ref_objs:\n",
    "            in_dict[key] = ref_objs[key]\n",
    "        tumor = ''\n",
    "        normal = ''\n",
    "        for stype in bam_dict[case_id]:\n",
    "            if stype == 'Normal':\n",
    "                in_dict['input_normal'] = bam_dict[case_id][stype]['file_obj']\n",
    "                normal = bam_dict[case_id][stype]['sid']\n",
    "            else:\n",
    "                in_dict['input_tumor'] = bam_dict[case_id][stype]['file_obj']\n",
    "                tumor = bam_dict[case_id][stype]['sid']\n",
    "                in_dict['sample_name'] = tumor\n",
    "        in_dict['sex'] = sex_dict[case_id]\n",
    "        in_dict['b_allele'] = b_dict[case_id]\n",
    "        task_name = 'CONTROLFREEC NO CONTAM ADJUST PLOIDY 2-4: ' + case_id + ' ' + tumor + ' ' + normal\n",
    "        # pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id + \"CONTAM_ADJUST\"\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error: \" + case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Run on PNOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found: cbttc-dna-somatic-BS_CGXTFM67_BS_6GS4XT7F-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_3Z40EZHD_BS_MVYA262V-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_JRFVST47_BS_MVYA262V-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_J8EK6RNF_BS_HJ7HYZ7N-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_M5FM63EB_BS_9H6Z0MEG-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_M0B42FPR_BS_9H6Z0MEG-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_9P4NDTKJ_BS_9H6Z0MEG-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_YZD4SSMA_BS_E5RKHG41-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_NNFDFAFM_BS_E5RKHG41-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_1MME7FBS_BS_STZ2C71Q-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_AHAXPFG3_BS_668350EZ-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_HEJ72V3F_BS_668350EZ-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_H8NWA41N_BS_Q7R8BT07-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_0ATJ22QA_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_DVDT4VXQ_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_YHXMYDBN_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_1Q524P3B_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_22VCR7DF_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_D6STCMQS_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_X5VN0FW0_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_AK9BV52G_BS_3PNWA7WT-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_7PF3C1P7_BS_QPSQPDR8-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_TQ0J7WJQ_BS_36YFSGDX-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_8SYN7GXG_BS_NY9MPC8F-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_FK3B5SDH_BS_Z370T42N-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_682Z7WH6_BS_9TSKXKGH-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_R6CKWZW6_BS_9TSKXKGH-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_BQ81D2BP_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_3VKW5988_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_HYKV2TH9_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_AF5D41PD_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_EE73VE7V_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_5968GBGT_BS_SNRF1RKC-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_7GKF6M85_BS_BKCPNFZ5-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_Q6GVRAWK_BS_D48QXYW6-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_VXDGXQKZ_BS_D48QXYW6-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_ZSH09N84_BS_29YQSB5E-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_CBMAWSAR_BS_29YQSB5E-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_J8EH1N7V_BS_29YQSB5E-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_Y74XAFJX_BS_29YQSB5E-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_QZRP3NSG_BS_9H6Z0MEG-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_6JBE0947_BS_STZ2C71Q-Reharmonization\n",
      "Valid task found: cbttc-dna-somatic-BS_4DQAQFQH_BS_36YFSGDX-Reharmonization\n"
     ]
    }
   ],
   "source": [
    "project = 'kfdrc-harmonization/sd-bhjxbdqk-09'\n",
    "app_name = project + '/kfdrc-controlfreec-wf'\n",
    "manifest = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/cram-manifest.csv')\n",
    "sex_prediction = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/2019-09-30-pbta-clin-update.tsv')\n",
    "b_allele = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/germline_calls-manifest.csv')\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "prefix = \"cbttc-dna-somatic\"\n",
    "suffix = \"Reharmonization\"\n",
    "tn_pairs = []\n",
    "bs_ids = []\n",
    "for task in tasks:\n",
    "    if re.search(prefix, task.name) and re.search(suffix, task.name):\n",
    "        sys.stderr.write(\"Valid task found: \" + task.name + \"\\n\")\n",
    "        tumor_id = task.inputs['tumor_id']\n",
    "        normal_id = task.inputs['normal_id']\n",
    "        tn_pairs.append([tumor_id, normal_id])\n",
    "        bs_ids.append(tumor_id)\n",
    "        if normal_id not in bs_ids:\n",
    "            bs_ids.append(normal_id)\n",
    "\n",
    "ref_objs = get_cf_refs(api)\n",
    "head = next(manifest)\n",
    "\n",
    "cram_dict = {}\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    sample_id = info[12]\n",
    "    fid = info[0]\n",
    "    cram_dict[sample_id] = api.files.get(fid)\n",
    "    \n",
    "sex_dict = {}\n",
    "s_trans = {\"Male\": \"XY\", \"Female\": \"XX\"}\n",
    "head = next(sex_prediction)\n",
    "for line in sex_prediction:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    bs_id = info[1]\n",
    "    pred_sex = info[-4]\n",
    "    ds_sex = info[7]\n",
    "    if bs_id in bs_ids and info[3] != \"Tumor\":\n",
    "        if pred_sex != \"NA\" and pred_sex == ds_sex:\n",
    "            sex_dict[bs_id] = s_trans[pred_sex]\n",
    "        else:\n",
    "            sys.stderr.write(\"Warn, prediction for \" + case_id + \" was inconclusive.  Default to reported sex\\n\")\n",
    "            sex_dict[case_id] = s_trans[ds_sex]\n",
    "b_dict = {}\n",
    "head = next(b_allele)\n",
    "for line in b_allele:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    bs_id = info[11]\n",
    "    b_dict[bs_id] = api.files.get(info[0])\n",
    "for pair in tn_pairs:\n",
    "    (tumor, normal) = (pair[0], pair[1])\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        for key in ref_objs:\n",
    "            in_dict[key] = ref_objs[key]\n",
    "        in_dict['input_normal'] = cram_dict[normal]\n",
    "        in_dict['input_tumor'] = cram_dict[tumor]\n",
    "        in_dict['sample_name'] = tumor\n",
    "        in_dict['sex'] = sex_dict[normal]\n",
    "        in_dict['b_allele'] = b_dict[normal]\n",
    "        task_name = 'CONTROLFREEC REHARMONIZATION: ' + tumor + ' ' + normal\n",
    "        # pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error: \" + tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run on PNOC Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'brownm28/kfdrc-purity-ploidy-dev'\n",
    "app_name = project + '/kfdrc-controlfreec-wf'\n",
    "manifest = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/dev_project_crams-manifest.csv')\n",
    "sex_prediction = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/2019-09-30-pbta-clin-update.tsv')\n",
    "b_allele = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/dev_project_germline-manifest.csv')\n",
    "pair_file = open('/Users/brownm28/Documents/2019-Oct-7_pnoc_cfree/curley_bs_id_pairs.txt')\n",
    "tn_pairs = []\n",
    "bs_ids = []\n",
    "head = next(pair_file)\n",
    "\n",
    "for line in pair_file:\n",
    "    (tumor_id, normal_id) = line.rstrip('\\n').split('\\t')\n",
    "    tn_pairs.append([tumor_id, normal_id])\n",
    "    bs_ids.append(tumor_id)\n",
    "    if normal_id not in bs_ids:\n",
    "        bs_ids.append(normal_id)\n",
    "\n",
    "ref_objs = get_cf_refs(api)\n",
    "head = next(manifest)\n",
    "\n",
    "cram_dict = {}\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    sample_id = info[12]\n",
    "    fid = info[0]\n",
    "    cram_dict[sample_id] = api.files.get(fid)\n",
    "    \n",
    "sex_dict = {}\n",
    "s_trans = {\"Male\": \"XY\", \"Female\": \"XX\"}\n",
    "head = next(sex_prediction)\n",
    "for line in sex_prediction:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    bs_id = info[1]\n",
    "    pred_sex = info[-4]\n",
    "    ds_sex = info[7]\n",
    "    if bs_id in bs_ids and info[3] != \"Tumor\":\n",
    "        if pred_sex != \"NA\" and pred_sex == ds_sex:\n",
    "            sex_dict[bs_id] = s_trans[pred_sex]\n",
    "        else:\n",
    "            sys.stderr.write(\"Warn, prediction for \" + case_id + \" was inconclusive.  Default to reported sex\\n\")\n",
    "            sex_dict[case_id] = s_trans[ds_sex]\n",
    "b_dict = {}\n",
    "head = next(b_allele)\n",
    "for line in b_allele:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    bs_id = info[11]\n",
    "    b_dict[bs_id] = api.files.get(info[0])\n",
    "for pair in tn_pairs:\n",
    "    (tumor, normal) = (pair[0], pair[1])\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        for key in ref_objs:\n",
    "            in_dict[key] = ref_objs[key]\n",
    "        in_dict['input_normal'] = cram_dict[normal]\n",
    "        in_dict['input_tumor'] = cram_dict[tumor]\n",
    "        in_dict['sample_name'] = tumor\n",
    "        in_dict['sex'] = sex_dict[normal]\n",
    "        in_dict['b_allele'] = b_dict[normal]\n",
    "        task_name = 'CFREEC PNOC CONTAM ADJ: ' + tumor + ' ' + normal\n",
    "        # pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id + '.pnoc_contam_adj'\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error: \" + tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNVkit Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckit_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['annotation_file'] = api.files.query(project=project, names=['refFlat_HG38.txt'])[0]\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['include_expression'] = 'FILTER=\"PASS\"'\n",
    "    ref_dict['wgs_mode'] = 'Y'\n",
    "    ref_dict['threads'] = 36\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = project + '/kfdrc-cnvkit-batch-wf'\n",
    "manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_crams-manifest.csv')\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "# case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/missed_case_list.txt')\n",
    "sex_prediction = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_sex_info_w_case_id.txt')\n",
    "b_allele = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_ballele-manifest.csv')\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "ref_objs = get_ckit_refs(api)\n",
    "head = next(manifest)\n",
    "bam_dict = {}\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    stype = info[9]\n",
    "    sample_id = info[11]\n",
    "    fid = info[0]\n",
    "    if case_id not in bam_dict:\n",
    "        bam_dict[case_id] = {}\n",
    "    bam_dict[case_id][stype] = {}\n",
    "    bam_dict[case_id][stype]['sid'] = sample_id\n",
    "    bam_dict[case_id][stype]['file_obj'] = api.files.get(fid)\n",
    "    \n",
    "sex_dict = {}\n",
    "head = next(sex_prediction)\n",
    "for line in sex_prediction:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    case_id = info[4]\n",
    "    pred_sex = info[-1]\n",
    "    ds_sex = info[1]\n",
    "    if case_id in case_list:\n",
    "        if pred_sex != \"Unknown\" and pred_sex == ds_sex:\n",
    "            sex_dict[case_id] = pred_sex\n",
    "        else:\n",
    "            sys.stderr.write(\"Warn, prediction for \" + case_id + \" was inconclusive.  Default to reported sex\\n\")\n",
    "            sex_dict[case_id] = s_trans[ds_sex]\n",
    "b_dict = {}\n",
    "head = next(b_allele)\n",
    "for line in b_allele:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    b_dict[case_id] = api.files.get(info[0])\n",
    "for case_id in case_list:\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        for key in ref_objs:\n",
    "            in_dict[key] = ref_objs[key]\n",
    "        tumor = ''\n",
    "        normal = ''\n",
    "        for stype in bam_dict[case_id]:\n",
    "            if stype == 'Normal':\n",
    "                in_dict['input_control'] = bam_dict[case_id][stype]['file_obj']\n",
    "                normal = bam_dict[case_id][stype]['sid']\n",
    "            else:\n",
    "                in_dict['input_sample'] = bam_dict[case_id][stype]['file_obj']\n",
    "                tumor = bam_dict[case_id][stype]['sid']\n",
    "                in_dict['tumor_sample_name'] = tumor\n",
    "        in_dict['sex'] = sex_dict[case_id]\n",
    "        in_dict['b_allele_vcf'] = b_dict[case_id]\n",
    "        task_name = 'CNVKIT FIRST PASS RERUN: ' + case_id + ' ' + tumor + ' ' + normal\n",
    "        #pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, execution_settings={'use_memoization': True}, run=False)\n",
    "        task.inputs['output_basename'] = task.id + \"_FIRST_PASS\"\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error:\" + case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found PureCN CNVKIT SEG INPUT: PASTGD03 BS_W1R54A2M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'id'\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASTGD03 BS_W1R54A2M due to error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found PureCN CNVKIT SEG INPUT: PASXRJ03 BS_KXRFQF5N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'id'\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASXRJ03 BS_KXRFQF5N due to error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found PureCN CNVKIT SEG INPUT: PASUTC03 BS_B80Z459E\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASFKX03 BS_2X9EVKZ0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'id'\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASUTC03 BS_B80Z459E due to error\n",
      "Requested file does not exist.\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASUTC03 BS_B80Z459E due to error\n",
      "Requested file does not exist.\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASUTC03 BS_B80Z459E due to error\n",
      "'NoneType' object has no attribute 'id'\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASFKX03 BS_2X9EVKZ0 due to error\n",
      "Requested file does not exist.\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASFKX03 BS_2X9EVKZ0 due to error\n",
      "Requested file does not exist.\n",
      "Skipping PureCN CNVKIT SEG INPUT: PASFKX03 BS_2X9EVKZ0 due to error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found PureCN CNVKIT SEG INPUT: PARUTJ03 BS_K2K5YSDS\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASCWD03 BS_DWYR5CTE\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASGRL03 BS_EQE447QB\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASMET03 BS_RA5HNMDP\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASUTC03 BS_B80Z459E\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PARTRP03 BS_4RX1AAVV\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASFKX03 BS_2X9EVKZ0\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASPGB03 BS_ACCE0MEA\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASUMG03 BS_WSK7MH3C\n",
      "Valid task found PureCN CNVKIT SEG INPUT: PASVCK03 BS_KQHSSRW3\n"
     ]
    }
   ],
   "source": [
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "# task name search phrase\n",
    "phrase = \"PureCN CNVKIT SEG INPUT:\"\n",
    "# modify this to set which input file to use to tag the outputs with, may need to modify code if an array element\n",
    "in_key = 'input_seg_file'\n",
    "for task in tasks:\n",
    "    if re.search(phrase, task.name):\n",
    "        sys.stderr.write('Valid task found ' + task.name + '\\n')\n",
    "        metadata = task.inputs[in_key].metadata\n",
    "        for out_key in task.outputs:\n",
    "            try:\n",
    "                if type(task.outputs[out_key]) is not list:\n",
    "                    file_obj = api.files.get(task.outputs[out_key].id)\n",
    "                    for key in metadata:\n",
    "                        file_obj.metadata[key] = metadata[key]\n",
    "                    file_obj.save()\n",
    "                else:\n",
    "                    for output in task.outputs[out_key]:\n",
    "                        file_obj = api.files.get(output.id)\n",
    "                        for key in metadata:\n",
    "                            file_obj.metadata[key] = metadata[key]\n",
    "                        file_obj.save()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Skipping \" + task.name + \" due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag batch outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 0558259d-187f-4556-8304-806049897818.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 2a6cd08a-09cc-4ee5-8690-a6c2abbc2253.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 08bdf6d1-26b4-4be6-b268-7851a98d6e30.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 2456132b-684c-490f-9d79-c5da945e6338.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 7231ef87-c951-4d7b-88ab-bb596a439135.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 348391d8-1dc2-4280-a59d-1122f3219c7f.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: ccb1acd3-059b-4641-838a-d38255c8d221.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 491538c3-12f7-455c-aeff-c0a3f25e0d02.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: eed069fe-4b39-4a38-9ba3-2d92b4fb5f4e.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: bd178ea2-9e2b-4858-ab8d-7fe9c90c67c5.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 6dd82a09-08d2-42c0-979a-4d7d638db361.postCGP.Gfiltered.vcf.gz\n",
      "Valid task found bcftools-filter-vcf run - 09-30-19 14:16:32: file: 65ed3ead-ecd1-43fc-af61-dc0e8764955d.postCGP.Gfiltered.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "batch_id = '818abb02-b61d-4313-a451-27c5d27cd4e7'\n",
    "batch_task = api.tasks.get(batch_id)\n",
    "for task in batch_task.get_batch_children():\n",
    "    # pdb.set_trace()\n",
    "    sys.stderr.write('Valid task found ' + task.name + '\\n')\n",
    "    metadata = task.inputs['input_vcf'].metadata\n",
    "    for out_key in task.outputs:\n",
    "        try:\n",
    "            file_obj = api.files.get(task.outputs[out_key].id)\n",
    "            for key in metadata:\n",
    "                file_obj.metadata[key] = metadata[key]\n",
    "            file_obj.save()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Skipping \" + task.name + \" due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PureCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcn_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['purecn_gc_ref'] = api.files.query(project=project, names=['hg38_PureCN_150bp_gc_file.txt'])[0]\n",
    "    ref_dict['dbsnp_vcf'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dbsnp138.vcf.gz'])[0]\n",
    "    ref_dict['include_expression'] = 'FILTER=\"PASS\" && (INFO/STATUS=\"Germline\" | INFO/STATUS=\"StrongSomatic\")'\n",
    "    ref_dict['genome_version'] = 'hg38'\n",
    "    ref_dict['cores'] = 8\n",
    "    ref_dict['max_segments'] = 2000\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = project + '/kfdrc-purecn-wf'\n",
    "seg_manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/cnvkit_seg-manifest.csv')\n",
    "# seg_manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/cfree_unadjusted_seg-manifest.csv')\n",
    "# case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/missed_case_list.txt')\n",
    "sex_prediction = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_sex_info_w_case_id.txt')\n",
    "somatic_germline = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_vardict_vcfs-manifest.csv')\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "ref_objs = get_pcn_refs(api)\n",
    "head = next(seg_manifest)\n",
    "seg_dict = {}\n",
    "for line in seg_manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    sample_id = info[11]\n",
    "    fid = info[0]\n",
    "    seg_dict[case_id] = api.files.get(fid)\n",
    "    \n",
    "sex_dict = {}\n",
    "s_trans = {\"Male\": \"M\", \"Female\": \"F\"}\n",
    "head = next(sex_prediction)\n",
    "for line in sex_prediction:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    case_id = info[4]\n",
    "    pred_sex = info[-1]\n",
    "    ds_sex = info[1]\n",
    "    if case_id in case_list:\n",
    "        if pred_sex != \"Unknown\" and pred_sex == ds_sex:\n",
    "            sex_dict[case_id] = s_trans[pred_sex]\n",
    "        else:\n",
    "            sys.stderr.write(\"Warn, prediction for \" + case_id + \" was inconclusive.  Default to reported sex\\n\")\n",
    "            sex_dict[case_id] = s_trans[ds_sex]\n",
    "b_dict = {}\n",
    "head = next(somatic_germline)\n",
    "for line in somatic_germline:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    b_dict[case_id] = api.files.get(info[0])\n",
    "for case_id in case_list:\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        for key in ref_objs:\n",
    "            in_dict[key] = ref_objs[key]\n",
    "        seg_obj = seg_dict[case_id]\n",
    "        tumor_sample_id = seg_obj.metadata['Kids First Biospecimen ID']\n",
    "        in_dict['input_seg_file'] = seg_obj\n",
    "        in_dict['tumor_sample_id'] = tumor_sample_id\n",
    "        in_dict['sex'] = sex_dict[case_id]\n",
    "        in_dict['somatic_germline_vcf'] = b_dict[case_id]\n",
    "        task_name = 'PureCN CNVKIT SEG 2K: ' + case_id + ' ' + tumor_sample_id\n",
    "        #pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, execution_settings={'use_memoization': True}, run=False)\n",
    "        task.inputs['output_basename'] = task.id + \"_CNVKIT_2KSEG\"\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error:\" + case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = project + '/cnvkit-theta2-wf'\n",
    "cns_manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/cns_segment-manifest.csv')\n",
    "cnn_manifest = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/cnn_cnvkit-manifest.csv')\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "somatic_germline = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_vardict_vcfs-manifest.csv')\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "head = next(cns_manifest)\n",
    "cns_dict = {}\n",
    "for line in cns_manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    fid = info[0]\n",
    "    cns_dict[case_id] = api.files.get(fid)\n",
    "head = next(cnn_manifest)\n",
    "cnn_dict = {}\n",
    "for line in cnn_manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    fid = info[0]\n",
    "    cnn_dict[case_id] = api.files.get(fid)\n",
    "b_dict = {}\n",
    "tum_id_list = {}\n",
    "norm_id_list = {}\n",
    "head = next(somatic_germline)\n",
    "for line in somatic_germline:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    case_id = info[-2]\n",
    "    norm_id = info[15]\n",
    "    tum_id = info[18]\n",
    "    b_dict[case_id] = api.files.get(info[0])\n",
    "    tum_id_list[case_id] = tum_id\n",
    "    norm_id_list[case_id] = norm_id\n",
    "for case_id in case_list:\n",
    "    try:\n",
    "        in_dict = {}\n",
    "        in_dict['include_expression'] = 'FILTER=\"PASS\" && (INFO/STATUS=\"Germline\" | INFO/STATUS=\"StrongSomatic\")'\n",
    "        in_dict['tumor_cns'] = cns_dict[case_id]\n",
    "        in_dict['reference_cnn'] = cnn_dict[case_id]\n",
    "        in_dict['tumor_ID'] = tum_id_list[case_id]\n",
    "        in_dict['normal_ID'] = norm_id_list[case_id]\n",
    "        in_dict['paired_vcf'] = b_dict[case_id]\n",
    "        task_name = 'Theta2 Run: ' + case_id + ' ' + tumor_sample_id\n",
    "        #pdb.set_trace()\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Skipping due to error:\" + case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found valid file 64ad5224-55a2-4b87-bc01-b4038a917749NO_CONTAM_ADJUST.controlfreec.ratio.png PT_9QQ37AWW Tumor\n",
      "Found valid file e41f128c-0e82-41dd-82c8-436601cbb9ceNO_CONTAM_ADJUST.controlfreec.ratio.png PT_1EQHANKW Tumor\n",
      "Found valid file 189e15ee-3d00-4840-b5c7-e222aba63b3cNO_CONTAM_ADJUST.controlfreec.ratio.png PT_TAJJ9MYY Tumor\n",
      "Found valid file fa7d814d-3912-49a3-82fa-8839955cdf0dNO_CONTAM_ADJUST.controlfreec.ratio.png PT_BXYKW39H Tumor\n",
      "Found valid file 173cb67f-3f85-4191-8e31-f1ba1cc45073NO_CONTAM_ADJUST.controlfreec.ratio.png PT_53M7K3JE Tumor\n",
      "Found valid file a1af172e-2f84-433c-9601-92497d9d9ecdNO_CONTAM_ADJUST.controlfreec.ratio.png PT_1YAJEAMJ Tumor\n",
      "Found valid file 7e0ebe74-52dd-41f1-a711-9d302bb6a157NO_CONTAM_ADJUST.controlfreec.ratio.png PT_R2TRGY6N Tumor\n",
      "Found valid file ba4fade5-c88b-4087-9dc7-f6e3e11d61aeNO_CONTAM_ADJUST.controlfreec.ratio.png PT_KWRFGRER Tumor\n",
      "Found valid file 08e87f40-a06d-46ab-93c0-73c31c245c98NO_CONTAM_ADJUST.controlfreec.ratio.png PT_ASH4P45D Tumor\n",
      "Found valid file 2e968d6b-fe4d-49d1-9f26-05cfd8373da8NO_CONTAM_ADJUST.controlfreec.ratio.png PT_3WF5J3PZ Tumor\n",
      "Found valid file f6c8b2e4-de42-4157-a302-082ede8f1e26NO_CONTAM_ADJUST.controlfreec.ratio.png PT_JF62ZBX8 Tumor\n",
      "Found valid file 57cc5803-b817-4324-8e57-c5a4b4d2ab60NO_CONTAM_ADJUST.controlfreec.ratio.png PT_GERVNCVY Tumor\n"
     ]
    }
   ],
   "source": [
    "maris_project = 'kfdrc-harmonization/sd-dypmehhf-05'\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "qual_tags = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/JR_qual_assign.txt')\n",
    "find_ext = 'NO_CONTAM_ADJUST.controlfreec.ratio.png$'\n",
    "name_ext = \"_contam_unadjusted.png\"\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "#pdb.set_trace()\n",
    "qual_dict = {}\n",
    "for line in qual_tags:\n",
    "    (case_id, qual) = line.rstrip('\\n').split('\\t')\n",
    "    qual_dict[case_id] = qual\n",
    "qual_tags.close()\n",
    "\n",
    "for case_id in case_list:\n",
    "    files = api.files.query(project=project, metadata = {'case_id': case_id} )\n",
    "    for file_obj in files:\n",
    "        if re.search(find_ext, file_obj.name):\n",
    "            print(\"Found valid file \" + file_obj.name + \" \" + file_obj.metadata[\"Kids First Participant ID\"] + \" \" + file_obj.metadata[\"sample_type\"])\n",
    "            file_obj.download('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/controlfreec/comparisons/pngs/' + case_id + \"_\" + qual_dict[case_id] + name_ext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather and parse ControlFreeC info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found valid file 3bfc3ac3-537b-4286-9d9a-845c95d0ba1dCONTAM_ADJUST.controlfreec.info.txt PT_9QQ37AWW Tumor\n",
      "Found valid file 64ad5224-55a2-4b87-bc01-b4038a917749NO_CONTAM_ADJUST.controlfreec.info.txt PT_9QQ37AWW Tumor\n",
      "Found valid file e41f128c-0e82-41dd-82c8-436601cbb9ceNO_CONTAM_ADJUST.controlfreec.info.txt PT_1EQHANKW Tumor\n",
      "Found valid file ebc12216-22e6-41f7-b6e1-f6da22cc0900CONTAM_ADJUST.controlfreec.info.txt PT_1EQHANKW Tumor\n",
      "Found valid file 189e15ee-3d00-4840-b5c7-e222aba63b3cNO_CONTAM_ADJUST.controlfreec.info.txt PT_TAJJ9MYY Tumor\n",
      "Found valid file 3bfc8300-0a04-4c90-b6d9-1bc9cc693dacCONTAM_ADJUST.controlfreec.info.txt PT_TAJJ9MYY Tumor\n",
      "Found valid file a410a847-0ac0-45f7-b971-f7196c0d7725CONTAM_ADJUST.controlfreec.info.txt PT_BXYKW39H Tumor\n",
      "Found valid file fa7d814d-3912-49a3-82fa-8839955cdf0dNO_CONTAM_ADJUST.controlfreec.info.txt PT_BXYKW39H Tumor\n",
      "Found valid file 173cb67f-3f85-4191-8e31-f1ba1cc45073NO_CONTAM_ADJUST.controlfreec.info.txt PT_53M7K3JE Tumor\n",
      "Found valid file 929f5e92-9c81-46f9-ae3d-714a458c7d6aCONTAM_ADJUST.controlfreec.info.txt PT_53M7K3JE Tumor\n",
      "Found valid file 25bb443b-9595-46af-89d8-15351219f46aCONTAM_ADJUST.controlfreec.info.txt PT_1YAJEAMJ Tumor\n",
      "Found valid file a1af172e-2f84-433c-9601-92497d9d9ecdNO_CONTAM_ADJUST.controlfreec.info.txt PT_1YAJEAMJ Tumor\n",
      "Found valid file 7e0ebe74-52dd-41f1-a711-9d302bb6a157NO_CONTAM_ADJUST.controlfreec.info.txt PT_R2TRGY6N Tumor\n",
      "Found valid file fc10062d-4aa6-460f-a70e-1517ee782f97CONTAM_ADJUST.controlfreec.info.txt PT_R2TRGY6N Tumor\n",
      "Found valid file 72dc557b-8b0f-4bd5-b00d-c832c5a1f2a6CONTAM_ADJUST.controlfreec.info.txt PT_KWRFGRER Tumor\n",
      "Found valid file ba4fade5-c88b-4087-9dc7-f6e3e11d61aeNO_CONTAM_ADJUST.controlfreec.info.txt PT_KWRFGRER Tumor\n",
      "Found valid file 08e87f40-a06d-46ab-93c0-73c31c245c98NO_CONTAM_ADJUST.controlfreec.info.txt PT_ASH4P45D Tumor\n",
      "Found valid file e997568a-bff9-47bb-814b-73835b77d7d4CONTAM_ADJUST.controlfreec.info.txt PT_ASH4P45D Tumor\n",
      "Found valid file 2e968d6b-fe4d-49d1-9f26-05cfd8373da8NO_CONTAM_ADJUST.controlfreec.info.txt PT_3WF5J3PZ Tumor\n",
      "Found valid file e70537a1-47e3-4809-8d4e-fb8da0fec587CONTAM_ADJUST.controlfreec.info.txt PT_3WF5J3PZ Tumor\n",
      "Found valid file a4fc7c8e-bf96-4b02-b3ef-ef8c33ae698bCONTAM_ADJUST.controlfreec.info.txt PT_JF62ZBX8 Tumor\n",
      "Found valid file f6c8b2e4-de42-4157-a302-082ede8f1e26NO_CONTAM_ADJUST.controlfreec.info.txt PT_JF62ZBX8 Tumor\n",
      "Found valid file 57cc5803-b817-4324-8e57-c5a4b4d2ab60NO_CONTAM_ADJUST.controlfreec.info.txt PT_GERVNCVY Tumor\n",
      "Found valid file 7d679571-9953-4249-a021-e374208fc498CONTAM_ADJUST.controlfreec.info.txt PT_GERVNCVY Tumor\n"
     ]
    }
   ],
   "source": [
    "# maris_project = 'kfdrc-harmonization/sd-dypmehhf-05'\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "qual_tags = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/JR_qual_assign.txt')\n",
    "find_ext = 'CONTAM_ADJUST.controlfreec.info.txt$'\n",
    "# name_ext = \"_contam_unadjusted.png\"\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "#pdb.set_trace()\n",
    "qual_dict = {}\n",
    "for line in qual_tags:\n",
    "    (case_id, qual) = line.rstrip('\\n').split('\\t')\n",
    "    qual_dict[case_id] = qual\n",
    "qual_tags.close()\n",
    "out = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/controlfreec/comparisons/cf_purity_ploidy_out.txt', 'w')\n",
    "out.write(\"Case ID\\tContam Adjust\\tWindow Size\\tPloidy\\tPurity\\n\")\n",
    "for case_id in case_list:\n",
    "    files = api.files.query(project=project, metadata = {'case_id': case_id} )\n",
    "    for file_obj in files:\n",
    "        if re.search(find_ext, file_obj.name):\n",
    "            print(\"Found valid file \" + file_obj.name + \" \" + file_obj.metadata[\"Kids First Participant ID\"] + \" \" + file_obj.metadata[\"sample_type\"])\n",
    "            data = file_obj.content().split(\"\\n\")\n",
    "            rtype = file_obj.tags[0]\n",
    "            # 7, 10, 11\n",
    "            (field, window) = data[7].split('\\t')\n",
    "            (field, ploidy) = data[10].split('\\t')\n",
    "            (field, purity) = data[11].split('\\t')\n",
    "            out.write(\"\\t\".join((case_id, rtype, window, ploidy, purity)) + \"\\n\")\n",
    "out.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather and Parse theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found valid file 16364a01-9d3e-4f3b-b5f5-94503226a853.BEST.results PT_9QQ37AWW Tumor\n",
      "Found valid file 6675d5a6-9836-4e6f-80f2-be217ab73161.BEST.results PT_1EQHANKW Tumor\n",
      "Found valid file dd6fa34b-66be-4b7e-b92a-175a5f197bb8.BEST.results PT_TAJJ9MYY Tumor\n",
      "Found valid file b2ca5cec-352a-4698-9bfe-c34126b9cace.BEST.results PT_BXYKW39H Tumor\n",
      "Found valid file c95e86a5-10e0-46f7-87d6-19b33b51ced0.BEST.results PT_53M7K3JE Tumor\n",
      "Found valid file 6aab8d01-ca9a-4a04-b93c-b94f44a2a994.BEST.results PT_1YAJEAMJ Tumor\n",
      "Found valid file 64d9cc2c-646d-4dc4-89a1-d690ddfe5ffd.BEST.results PT_R2TRGY6N Tumor\n",
      "Found valid file 003daa8a-b59f-488c-8606-64754899e990.BEST.results PT_KWRFGRER Tumor\n",
      "Found valid file 5b30afc1-1638-4115-bf24-4d6aae519060.BEST.results PT_ASH4P45D Tumor\n",
      "Found valid file 505f4a0b-8266-4a42-893b-08e12d7a0aa9.BEST.results PT_3WF5J3PZ Tumor\n",
      "Found valid file e1b44d7b-83f4-475e-90d0-a55e1196c376.BEST.results PT_JF62ZBX8 Tumor\n",
      "Found valid file 8b529177-3b35-4f44-adc9-bc3c4edd1613.BEST.results PT_GERVNCVY Tumor\n"
     ]
    }
   ],
   "source": [
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "qual_tags = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/JR_qual_assign.txt')\n",
    "find_ext = '.BEST.results$'\n",
    "# name_ext = \"_contam_unadjusted.png\"\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "#pdb.set_trace()\n",
    "qual_dict = {}\n",
    "for line in qual_tags:\n",
    "    (case_id, qual) = line.rstrip('\\n').split('\\t')\n",
    "    qual_dict[case_id] = qual\n",
    "qual_tags.close()\n",
    "out = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/controlfreec/comparisons/theta2_purity_out.txt', 'w')\n",
    "out.write(\"Case ID\\tPurity\\n\")\n",
    "for case_id in case_list:\n",
    "    files = api.files.query(project=project, metadata = {'case_id': case_id} )\n",
    "    for file_obj in files:\n",
    "        if re.search(find_ext, file_obj.name):\n",
    "            print(\"Found valid file \" + file_obj.name + \" \" + file_obj.metadata[\"Kids First Participant ID\"] + \" \" + file_obj.metadata[\"sample_type\"])\n",
    "            data = file_obj.content().split(\"\\n\")\n",
    "            purity = data[1].split('\\t')[1].split(',')\n",
    "            n2 = round(float(purity[1]),2)\n",
    "            if len(purity) > 2:\n",
    "                n3 = round(float(purity[2]),2)\n",
    "                purity = str(n2) + \" + \" + str(n3) + \" = \" + str(round(n2 + n3,2))\n",
    "            else:\n",
    "                purity = str(n2)\n",
    "            out.write(\"\\t\".join((case_id, purity)) + \"\\n\")\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get insert size metrics of crams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_str(val):\n",
    "    return str(round(float(val), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "germ_origin = 'kfdrc-harmonization/sd-dypmehhf'\n",
    "tumor_origin= 'kfdrc-harmonization/sd-dypmehhf-03/'\n",
    "\n",
    "case_id_list = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/maris_test_set.txt')\n",
    "qual_tags = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/JR_qual_assign.txt')\n",
    "\n",
    "qual_dict = {}\n",
    "for line in qual_tags:\n",
    "    (case_id, qual) = line.rstrip('\\n').split('\\t')\n",
    "    qual_dict[case_id] = qual\n",
    "qual_tags.close()\n",
    "\n",
    "case_list = []\n",
    "for line in case_id_list:\n",
    "    case_list.append(line.rstrip('\\n'))\n",
    "case_id_list.close()\n",
    "out = open('/Users/brownm28/Documents/2019-Aug-23_cnv_ts/benchmark_run/insert_size_metrics.txt', 'w')\n",
    "out.write(\"Case ID\\tSample Type\\tJR Assess\\tPT ID\\tBS ID\\tMedian Insert Size\\tMean Insert Size\\tStd Dev Insert Size\\tMean Coverage\\tStd Dev Coverage\\n\")\n",
    "res_dict = {}\n",
    "row = 7\n",
    "for case_id in case_list:\n",
    "    res_dict[case_id] = {}\n",
    "    stype = \"Tumor\"\n",
    "    res_dict[case_id][stype] = {}\n",
    "    tumor_files = api.files.query(project=tumor_origin, metadata = {'case_id': case_id} )\n",
    "    for file_obj in tumor_files:\n",
    "        if re.search(\".cram$\", file_obj.name):\n",
    "            # pdb.set_trace()\n",
    "            ids = file_obj.metadata[\"Kids First Participant ID\"] + \"\\t\" + file_obj.metadata[\"Kids First Biospecimen ID\"]\n",
    "            res_dict[case_id][stype]['ids'] = ids\n",
    "            task = api.tasks.get(file_obj.name.split('.')[0])\n",
    "            wgs = api.files.get(task.outputs['wgs_metrics'].id)\n",
    "            wgs_data = wgs.content().split('\\n')\n",
    "            wgs_info = wgs_data[row].split('\\t')\n",
    "            res_dict[case_id][stype]['xcov'] = rounded_str(wgs_info[1])\n",
    "            res_dict[case_id][stype]['scov'] = rounded_str(wgs_info[2])\n",
    "\n",
    "            for out_file in task.outputs['aggregation_metrics']:\n",
    "                if re.search(\".insert_size_metrics$\", out_file.name):\n",
    "                    ins = api.files.get(out_file.id)\n",
    "                    ins_data = ins.content().split('\\n')\n",
    "                    ins_info = ins_data[row].split('\\t')\n",
    "                    res_dict[case_id][stype]['mins'] = rounded_str(ins_info[0])\n",
    "                    res_dict[case_id][stype]['xins'] = rounded_str(ins_info[5])\n",
    "                    res_dict[case_id][stype]['sins'] = rounded_str(ins_info[6])\n",
    "    stype = \"Normal\"\n",
    "    res_dict[case_id][stype] = {}\n",
    "    normal_files = api.files.query(project=germ_origin, metadata = {'case_id': case_id} )\n",
    "    for file_obj in normal_files:\n",
    "        if re.search(\".cram$\", file_obj.name):\n",
    "            ids = file_obj.metadata[\"Kids First Participant ID\"] + \"\\t\" + file_obj.metadata[\"Kids First Biospecimen ID\"]\n",
    "            res_dict[case_id][stype]['ids'] = ids\n",
    "            task = api.tasks.get(file_obj.name.split('.')[0])\n",
    "            wgs = api.files.get(task.outputs['wgs_metrics'].id)\n",
    "            wgs_data = wgs.content().split('\\n')\n",
    "            wgs_info = wgs_data[row].split('\\t')\n",
    "            res_dict[case_id][stype]['xcov'] = rounded_str(wgs_info[1])\n",
    "            res_dict[case_id][stype]['scov'] = rounded_str(wgs_info[2])\n",
    "\n",
    "            for out_file in task.outputs['aggregation_metrics']:\n",
    "                if re.search(\".insert_size_metrics$\", out_file.name):\n",
    "                    ins = api.files.get(out_file.id)\n",
    "                    ins_data = ins.content().split('\\n')\n",
    "                    ins_info = ins_data[row].split('\\t')\n",
    "                    res_dict[case_id][stype]['mins'] = rounded_str(ins_info[0])\n",
    "                    res_dict[case_id][stype]['xins'] = rounded_str(ins_info[5])\n",
    "                    res_dict[case_id][stype]['sins'] = rounded_str(ins_info[6])\n",
    "for case_id in case_list:\n",
    "    for stype in res_dict[case_id]:\n",
    "        cur = res_dict[case_id][stype]\n",
    "        try:\n",
    "            out.write(\"\\t\".join([case_id, stype, qual_dict[case_id], cur['ids'], cur['mins'], cur['xins'], cur['sins'], cur['xcov'], cur['scov']]) + '\\n')\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            pdb.set_trace()\n",
    "            hold =1\n",
    "            \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Array to CFREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "seg_file = open(sys.argv[1])\n",
    "qual_tags = open(sys.argv[2])\n",
    "manifest = open(sys.argv[3])\n",
    "out_pre = sys.argv[4]\n",
    "flag = int(sys.argv[5])\n",
    "qual_dict = {}\n",
    "for line in qual_tags:\n",
    "    (case_id, qual) = line.rstrip('\\n').split('\\t')\n",
    "    qual_dict[case_id.replace(\"03\",\"\")] = qual\n",
    "qual_tags.close()\n",
    "\n",
    "array_calls = {}\n",
    "loss_lrr = -0.18\n",
    "gain_lrr = 0.3\n",
    "seg_out = open(\"seg_w_annot.seg\", \"w\")\n",
    "seg_out.write(\"Sample\\tChromosome\\tStart\\tEnd\\tNum_markers\\tSeg_CN\\tMB_call\")\n",
    "for line in seg_file:\n",
    "    (sample, chrom, start, end, p_ct, lrr) = line.rstrip('\\n').split('\\t')\n",
    "    status = \"neutral\"\n",
    "    if sample not in array_calls:\n",
    "        array_calls[sample] = {}\n",
    "    if chrom not in array_calls[sample]:\n",
    "            array_calls[sample][chrom] = []\n",
    "    if (float(lrr) <= loss_lrr or float(lrr) >= gain_lrr):\n",
    "        status = \"loss\"\n",
    "        if float(lrr) > 0:\n",
    "            status = \"gain\"\n",
    "    array_calls[sample][chrom].append([int(start) + 1, int(end), status])\n",
    "    seg_out.write(line.rstrip('\\n') + \"\\t\" + status + \"\\n\")\n",
    "seg_file.close()\n",
    "\n",
    "out_annot = open(out_pre + \"cfree_annotated_calls.txt\", \"w\")\n",
    "if flag == 0:\n",
    "    out_annot.write(\"Case ID\\tQual Tag\\tAnnot\\tchr\\tstart\\tend\\tcopy number\\tstatus\\tgenotype\\tuncertainty\\tWilcoxonRankSumTestPvalue\\tKolmogorovSmirnovPvalue\\n\")\n",
    "else:\n",
    "    out_annot.write(\"Case ID\\tQual Tag\\tAnnot\\tchr\\tstart\\tend\\tcopy number\\tstatus\\tWilcoxonRankSumTestPvalue\\tKolmogorovSmirnovPvalue\\n\")\n",
    "head = next(manifest)\n",
    "for metadata in manifest:\n",
    "    finfo = metadata.rstrip('\\n').split(',')\n",
    "    case_id = finfo[-2].replace(\"03\",\"\")\n",
    "    flag = 0\n",
    "    pval_fh = open(finfo[1])\n",
    "    head = next(pval_fh)\n",
    "    header = head.split('\\t')\n",
    "\n",
    "    for data in pval_fh:\n",
    "        out_annot.write(case_id + \"\\t\" + qual_dict[case_id])\n",
    "        cnv = data.split('\\t')\n",
    "        (chrom, start, end, status) = (cnv[0], cnv[1], cnv[2], cnv[4])\n",
    "        if case_id in array_calls and chrom in array_calls[case_id]:\n",
    "            f = 0\n",
    "            for coords in array_calls[case_id][chrom]:\n",
    "                if int(start) <= coords[1] and int(end) >= coords[0]:\n",
    "                    f = 1\n",
    "                    if status == coords[2]:\n",
    "                        out_annot.write(\"\\tTP\\t\" + data)\n",
    "                    else:\n",
    "                        out_annot.write(\"\\tMM\\t\" + data)\n",
    "                    break\n",
    "            if f == 0:\n",
    "                out_annot.write(\"\\tFP\\t\" + data)\n",
    "        else:\n",
    "            out_annot.write(\"\\tFP\\t\" + data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Filter Ratio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-4d00e633637a>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-4d00e633637a>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    draw_cmd.write(\"cat makeGraph.R | R --slave --args \" + ploidy_dict[case_id] + \" \" +  < *_ratio.txt >\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "pval_concat_file = open(sys.argv[1])\n",
    "info_manifest = open(sys.argv[2])\n",
    "ratio_manifest = open(sys.argv[3])\n",
    "len_min = int(sys.argv[4])\n",
    "out_pre = sys.argv[5]\n",
    "head = next(pval_concat_file)\n",
    "coords = {}\n",
    "failed = {}\n",
    "for line in pval_concat_file:\n",
    "    info = line.rstrip('\\n').split('\\t')\n",
    "    (case_id, chrom, start, end) = (info[0], info[3], int(info[4]), int(info[5]))\n",
    "    case_id += \"03\"\n",
    "    if case_id not in coords:\n",
    "        coords[case_id] = {}\n",
    "        failed[case_id] = 0\n",
    "    if chrom not in coords[case_id]:\n",
    "        coords[case_id][chrom] = []\n",
    "    if end - start >= len_min or chrom == \"Y\":\n",
    "        coords[case_id][chrom].append([start, end])\n",
    "    else:\n",
    "        failed[case_id] += 1\n",
    "ploidy_dict = {}\n",
    "head = next(info_manifest)\n",
    "ploidy_fh = open(out_pre + \"_cfree_ploidy.txt\", \"w\")\n",
    "for line in info_manifest:\n",
    "    metadata = line.rstrip('\\n').split(',')\n",
    "    case_id = metadata[-2]\n",
    "    cur = open(metadata[1])\n",
    "    for i in range(0, 10, 1):\n",
    "        skip = next(cur)\n",
    "    ploidy_value = next(cur)\n",
    "    val = ploidy_value.rstrip('\\n').split('\\t')[1]\n",
    "    ploidy_dict[case_id] = val\n",
    "    ploidy_fh.write(case_id + \"\\t\" + val + \"\\n\")\n",
    "    cur.close()\n",
    "sys.stderr.write(\"Case ID\\tSource\\tFailed GT \" + str(len_min) + \" CT\\n\")\n",
    "for case_id in failed:\n",
    "    sys.stderr.write(case_id + \"\\tP Val File\\t\" + str(failed[case_id]) + '\\n')\n",
    "\n",
    "head = next(ratio_manifest)\n",
    "draw_cmd = open(out_pre + \"_\" + str(len_min) + \"_cfree_draw.sh\", \"w\")\n",
    "for line in ratio_manifest:\n",
    "    metadata = line.rstrip('\\n').split(',')\n",
    "    case_id = metadata[-2]\n",
    "    new_ratio_fname = out_pre + \"_\" + case_id + \"_\" + str(len_min) + \".ratio.txt\"\n",
    "    filt_ratio = open(new_ratio_fname, \"w\")\n",
    "    draw_cmd.write(\"cat makeGraph.R | R --slave --args \" + ploidy_dict[case_id] + \" \" + new_ratio_fname + \"\\n\")\n",
    "    cur = open(metadata[1])\n",
    "    head = next(cur)\n",
    "    filt_ratio.write(head)\n",
    "    neutral = \"1\\t1\\t\" + ploidy_dict[case_id] + \"\\t-1\\t2\\t-\\t-1\"\n",
    "    fail_ct = 0\n",
    "    for data in cur:\n",
    "        datum = data.rstrip('\\n').split('\\t')\n",
    "        (chrom, pos) = (datum[0], datum[1])\n",
    "        flag = 0\n",
    "        if case_id in coords and chrom in coords[case_id]:\n",
    "            for entry in coords[case_id][chrom]:\n",
    "                if int(pos) >= entry[0] and int(pos) <= entry[1] or chrom == \"Y\":\n",
    "                    flag = 1\n",
    "                    filt_ratio.write(data)\n",
    "                    break\n",
    "                # end early in searching beyond end of coordinate\n",
    "                elif int(pos) > entry[1]:\n",
    "                    break\n",
    "            if flag == 0:\n",
    "                filt_ratio.write(\"\\t\".join([chrom, pos, neutral]) + '\\n')\n",
    "                fail_ct += 1\n",
    "        else:\n",
    "            filt_ratio.write(\"\\t\".join([chrom, pos, neutral]) + '\\n')\n",
    "            fail_ct += 1\n",
    "    filt_ratio.close()\n",
    "    sys.stderr.write(case_id + \"\\t Ratio file\\t\" + str(fail_ct) + '\\n')\n",
    "draw_cmd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add GUI Tag to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid task found SINGLE GENOTYPE GATK: PT_NK8A49X5 BS_668350EZ\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_MNSEJCDM BS_29YQSB5E\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KBFM551M BS_1GWZCWVG\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_M23Q0DC3 BS_6GS4XT7F\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_M9XXJ4GR BS_9TSKXKGH\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KBFM551M BS_9H6Z0MEG\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_HGM20MW7 BS_NY9MPC8F\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_1E3E6GMF BS_BKCPNFZ5\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_0MXPTTM3 BS_Z370T42N\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KZ56XHJT BS_3PNWA7WT\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KBFM551M BS_HJ7HYZ7N\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KZ56XHJT BS_Q7R8BT07\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_V1HNAC2Q BS_E5RKHG41\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_QA9WJ679 BS_QPSQPDR8\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_KTRJ8TFY BS_SNRF1RKC\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_WGVEF96B BS_36YFSGDX\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_NK8A49X5 BS_STZ2C71Q\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_9GKVQ9QS BS_MVYA262V\n",
      "Valid task found SINGLE GENOTYPE GATK: PT_VPEMAQBN BS_D48QXYW6\n"
     ]
    }
   ],
   "source": [
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "# task name search phrase\n",
    "phrase = \"SINGLE GENOTYPE GATK\"\n",
    "tags = ['GATK', 'GERMLINE', 'SNPEFF']\n",
    "# modify this to set which input file to use to tag the outputs with, may need to modify code if an array element\n",
    "out_keys = ['snpeff_vcf']\n",
    "for task in tasks:\n",
    "    if re.search(phrase, task.name):\n",
    "        sys.stderr.write('Valid task found ' + task.name + '\\n')\n",
    "        for out_key in out_keys:\n",
    "            try:\n",
    "                if type(task.outputs[out_key]) is not list:\n",
    "                    file_obj = api.files.get(task.outputs[out_key].id)\n",
    "                    file_obj.tags=tags\n",
    "                    file_obj.save()\n",
    "                else:\n",
    "                    for output in task.outputs[out_key]:\n",
    "                        file_obj = api.files.get(output.id)\n",
    "                        for key in metadata:\n",
    "                            file_obj.metadata[key] = metadata[key]\n",
    "                        file_obj.save()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Skipping \" + task.name + \" due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Controlfreec/CNKkit Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['annotation_file'] = api.files.query(project=project, names=['refFlat_HG38.txt'])[0]\n",
    "    ref_dict['indexed_reference_fasta'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['combined_include_expression'] = 'FILTER=\"PASS\" && (INFO/STATUS=\"Germline\" | INFO/STATUS=\"StrongSomatic\")'\n",
    "    ref_dict['wgs_mode'] = 'Y'\n",
    "    ref_dict['threads'] = 16\n",
    "    ref_dict['chr_len'] = api.files.query(project=project, names=['hs38_chr.len'])[0]\n",
    "    ref_dict['reference_fai'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta.fai'])[0]\n",
    "    ref_dict['coeff_var'] = 0.05\n",
    "    ref_dict['mate_orientation_control'] = \"FR\"\n",
    "    ref_dict['mate_orientation_sample'] = \"FR\"\n",
    "    ref_dict['ploidy'] = [2,3,4]\n",
    "    ref_dict['contamination_adjustment'] = False\n",
    "\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task: id=e3ec7d2c-c6f1-4798-85c3-c9f795d243d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = project + '/kfdrc-combined-somatic-wgs-cnv-wf'\n",
    "paired_vcf = api.files.get('5d83e9aee4b0e4c53a247ec8')\n",
    "b_allele = api.files.get('5db0b8c5e4b0950c6e16e760')\n",
    "ref_objs = get_refs(api)\n",
    "in_dict = {}\n",
    "for key in ref_objs:\n",
    "    in_dict[key] = ref_objs[key]\n",
    "in_dict['paired_vcf'] = paired_vcf\n",
    "in_dict['b_allele'] = b_allele\n",
    "in_dict['cfree_sex'] = \"XY\"\n",
    "in_dict['cnvkit_sex'] = \"Male\"\n",
    "in_dict[\"input_normal_name\"] = \"BS_2TZNPK1V\"\n",
    "in_dict[\"input_tumor_name\"] = \"BS_R3WB0PP7\"\n",
    "in_dict['input_tumor_aligned'] = api.files.get('5d83e922e4b0e4c53a247c38')\n",
    "in_dict['input_normal_aligned'] = api.files.get('5d83e922e4b0e4c53a247c35')\n",
    "in_dict['output_basename'] = 'COMBINED_CNV_SUBSET_TEST'\n",
    "task_name = \"COMBINED_CNV_SUBSET_TEST\"\n",
    "task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=in_dict, run=False)\n",
    "task.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
