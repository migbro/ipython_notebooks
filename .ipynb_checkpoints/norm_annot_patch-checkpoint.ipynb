{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize, Annotate\n",
    "This is a notebook to run the patch workflow to bring existing VCFs to spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sevenbridges as sbg\n",
    "from sevenbridges.errors import SbgError\n",
    "from sevenbridges.http.error_handlers import rate_limit_sleeper, maintenance_sleeper\n",
    "import sys\n",
    "import re\n",
    "import concurrent.futures\n",
    "import pdb\n",
    "config = sbg.Config(profile='turbo')\n",
    "api = sbg.Api(config=config, error_handlers=[rate_limit_sleeper, maintenance_sleeper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['bcftools_annot_vcf'] = api.files.query(project=project, names=[\"af-only-gnomad.hg38.vcf.gz\"])[0]\n",
    "    ref_dict['bcftools_public_filter'] = 'FILTER=\"PASS\"|INFO/HotSpotAllele=1'\n",
    "    ref_dict['bcftools_annot_columns'] = \"INFO/AF\"\n",
    "    ref_dict['disable_hotspot_annotation'] = False\n",
    "    ref_dict['gatk_filter_name'] = [\"NORM_DP_LOW\", \"GNOMAD_AF_HIGH\"]\n",
    "    ref_dict['vep_cache'] = api.files.query(project=project, names=[\"homo_sapiens_vep_93_GRCh38.tar.gz\"])[0]\n",
    "    ref_dict['genomic_hotspots'] = [api.files.query(project=project, names=[\"tert.bed\"])[0]]\n",
    "    ref_dict['protein_snv_hotspots'] = [api.files.query(project=project, names=[\"protein_snv_cancer_hotspots_v2.tsv\"])[0]]\n",
    "    ref_dict['protein_indel_hotspots'] = [api.files.query(project=project, names=[\"protein_indel_cancer_hotspots_v2.tsv\"])[0]]\n",
    "    ref_dict['indexed_reference_fasta'] = api.files.query(project=project, names=[\"Homo_sapiens_assembly38.fasta\"])[0]\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_task(ref_dict, tool_name, retain_info, vcf_obj, short_name, tumor_id, normal_id, project):\n",
    "    try:\n",
    "        input_dict = {}\n",
    "        for key in ref_dict:\n",
    "            input_dict[key] = ref_dict[key]\n",
    "        # pdb.set_trace()\n",
    "        if tool_name == \"strelka2_somatic\":\n",
    "            input_dict['add_common_fields'] = True\n",
    "        else:\n",
    "            input_dict['add_common_fields'] = False\n",
    "        input_dict['input_vcf'] = vcf_obj\n",
    "        input_dict['gatk_filter_expression'] = [\"vc.getGenotype('\" + normal_id + \"').getDP() <= 7\",\"AF > 0.001\"]\n",
    "        input_dict['tool_name'] = tool_name\n",
    "        input_dict['retain_info'] = retain_info\n",
    "        input_dict['input_tumor_name'] = tumor_id\n",
    "        input_dict['input_normal_name'] = normal_id\n",
    "        task_name = \"KFDRC NORM ANNOT PATCH RPT: \" + short_name + \" \" + tool_name + \" \" + tumor_id + \" \" + normal_id\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=input_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"had a problem drafting a task for \" + tumor_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_manifest(entry):\n",
    "    try:\n",
    "        info = entry.rstrip('\\n').split(',')\n",
    "        in_vcf = api.files.get(info[0])\n",
    "        tool_name = ''\n",
    "        # use keys to search file name for key word, then standardize tool name\n",
    "        for key in tool_name_dict:\n",
    "            if re.search(key, info[1]):\n",
    "                tool_name = tool_name_dict[key]\n",
    "                break\n",
    "        retain_info = retain_info_dict[tool_name]\n",
    "        draft_task(ref_dict, tool_name, retain_info, in_vcf, short_name, info[t_idx], info[n_idx], project)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"had a problem with parsing manifest for \" + entry)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out project, vcf_input and short_name vars not used\n",
    "pnoc_project = \"kfdrc-harmonization/sd-8y99qzjj-ad-hoc-caller-rerun\"\n",
    "pnoc_vcf_input = open('/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/vardict_rpt/pnoc_vardict_vcfs-manifest.csv')\n",
    "pnoc_short_name = \"PNOC\"\n",
    "\n",
    "# cbtn_project = \"kfdrc-harmonization/sd-bhjxbdqk-ad-hoc-caller-rerun\"\n",
    "# cbtn_vcf_input = open('/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/vardict_rpt/cbtn_vardict_vcfs-manifest.csv')\n",
    "# cbtn_short_name = \"CBTN\"\n",
    "\n",
    "# tcga_project = \"kfdrc-harmonization/openpbta-tcga-ad-hoc-caller-rerun\"\n",
    "# tcga_vcf_input = open('/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/tcga_input_vcf-manifest.csv')\n",
    "# tcga_short_name = \"TCGA\"\n",
    "\n",
    "# set these vars to the ones uncommented and to be used in the process\n",
    "project = pnoc_project\n",
    "vcf_input = pnoc_vcf_input\n",
    "short_name = pnoc_short_name\n",
    "\n",
    "app_name = project + \"/kfdrc-norm-annot-wf\"\n",
    "tool_name_dict = {'strelka': 'strelka2_somatic', 'mutect': 'mutect2_somatic',\n",
    "                  'lancet': 'lancet_somatic', 'vardict': 'vardict_somatic'}\n",
    "# use previous result to set helpful info output for maf fields\n",
    "retain_info_dict = {'strelka2_somatic': 'MQ,MQ0,QSI,HotSpotAllele',\n",
    "                   'mutect2_somatic': 'MBQ,TLOD,HotSpotAllele',\n",
    "                   'lancet_somatic':'MS,FETS,HotSpotAllele',\n",
    "                   'vardict_somatic': 'MSI,MSILEN,SOR,SSF,HotSpotAllele'}\n",
    "ref_dict = get_refs(api)\n",
    "head = next(vcf_input)\n",
    "header = head.rstrip('\\n').split(',')\n",
    "# get tumor and normal ID positions - commented because TCGA is slightly different\n",
    "t_idx = header.index('Kids First Biospecimen ID Tumor')\n",
    "n_idx = header.index('Kids First Biospecimen ID Normal')\n",
    "# t_idx = header.index('bam_tumor_id')\n",
    "# n_idx = header.index('bam_normal_id')\n",
    "i = 1\n",
    "n = 100\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(parse_manifest, line): line for line in vcf_input}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks created\\n')\n",
    "        i += 1\n",
    "\n",
    "# for line in vcf_input:\n",
    "#     parse_manifest(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey tasks from TCGA project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_task(task):\n",
    "    try:\n",
    "        if 'input_tumor_aligned' in task.inputs:\n",
    "            return [task.id, task.name, str(task.end_time.date()), str(task.end_time.time()),\n",
    "                    task.inputs[\"input_tumor_aligned\"].name,\n",
    "                    task.inputs[\"input_tumor_aligned\"].tags[0], task.app]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        sys.exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"cavatica/openpbta-tcga\"\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "out = open(\"/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/tcga_tasks.txt\", \"w\")\n",
    "out.write(\"\\t\".join([\"task id\", \"task name\", \"date\", \"time\", \"tumor bam\", \"bam tag\", \"app\"]) + \"\\n\")\n",
    "i = 1\n",
    "n = 100\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(parse_task, task): task for task in tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks processed\\n')\n",
    "        i += 1\n",
    "        if result.result() is not None:\n",
    "            out.write(\"\\t\".join(result.result()) + \"\\n\")\n",
    "out.close()\n",
    "\n",
    "\n",
    "# for task in tasks:\n",
    "#     info = parse_task(task)\n",
    "#     if info is not None:\n",
    "#         print(\"\\t\".join(info))\n",
    "#         pdb.set_trace()\n",
    "#         hold=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review TCGA metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_march_task(task):\n",
    "    try:\n",
    "        if 'input_tumor_aligned' in task.inputs and task.end_time.month == 3:\n",
    "            return [task.id, task.name,\n",
    "                    task.inputs[\"input_tumor_aligned\"].id,\n",
    "                    task.inputs[\"input_tumor_aligned\"].name,\n",
    "                   task.inputs[\"input_tumor_aligned\"].metadata['case_id'],\n",
    "                   task.inputs[\"input_tumor_aligned\"].metadata['sample_id'],\n",
    "                   task.inputs[\"input_tumor_aligned\"].metadata['aliquot_id'],\n",
    "                    task.inputs[\"input_tumor_name\"],\n",
    "                    task.inputs[\"input_normal_aligned\"].id,\n",
    "                    task.inputs[\"input_normal_aligned\"].name,\n",
    "                    task.inputs[\"input_normal_aligned\"].metadata['case_id'],\n",
    "                   task.inputs[\"input_normal_aligned\"].metadata['sample_id'],\n",
    "                   task.inputs[\"input_normal_aligned\"].metadata['aliquot_id'],\n",
    "                    task.inputs[\"input_normal_name\"]\n",
    "                   ]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        sys.exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"cavatica/openpbta-tcga\"\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "out = open(\"/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/tcga_bam_info.txt\", \"w\")\n",
    "out.write(\"\\t\".join([\"task id\", \"task name\", \"tumor bam id\",\n",
    "                     \"tumor bam name\", \"tumor bam case id\",\n",
    "                     \"tumor bam sample id\", \"tumor bam aliquot id\",\n",
    "                     \"input_tumor_name\", \"normal bam id\",\n",
    "                     \"normal bam name\", \"normal bam case id\",\n",
    "                     \"normal bam sample id\", \"normal bam aliquot id\",\n",
    "                     \"input_normal_name\"\n",
    "                    ]) + \"\\n\")\n",
    "i = 1\n",
    "n = 100\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(parse_march_task, task): task for task in tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks processed\\n')\n",
    "        i += 1\n",
    "        if result.result() is not None:\n",
    "            out.write(\"\\t\".join(result.result()) + \"\\n\")\n",
    "out.close()\n",
    "\n",
    "\n",
    "# for task in tasks:\n",
    "#     info = parse_march_task(task)\n",
    "#     if info is not None:\n",
    "#         print(\"\\t\".join(info))\n",
    "#         pdb.set_trace()\n",
    "#         hold=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch metadata to March outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_march_meta(task):\n",
    "    try:\n",
    "        if 'input_tumor_aligned' in task.inputs and task.end_time.month == 3:\n",
    "            metadata = {}\n",
    "            for key in task.inputs[\"input_tumor_aligned\"].metadata:\n",
    "                metadata[key] = task.inputs[\"input_tumor_aligned\"].metadata[key]\n",
    "            metadata['bam_normal_id'] = task.inputs[\"input_normal_name\"]\n",
    "            metadata['bam_tumor_id'] = task.inputs[\"input_tumor_name\"]\n",
    "            for key in task.outputs:\n",
    "                try:\n",
    "                    out_file_obj = api.files.get(task.outputs[key].id)\n",
    "                except Exception as e:\n",
    "                    print (e)\n",
    "                    print (\"Error getting file output for \" + task.name + \" \" + task.id + \" skipping!\")\n",
    "                    break\n",
    "                # if out_file_obj.metadata is None:\n",
    "                out_file_obj.metadata = metadata\n",
    "                out_file_obj.save()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Got an error processing \" + task.name + \" \" + task.id)\n",
    "\n",
    "        # sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"cavatica/openpbta-tcga\"\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "i = 1\n",
    "n = 100\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(parse_march_meta, task): task for task in tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks processed\\n')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add helpful tag for output processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_march_tag(task):\n",
    "    try:\n",
    "        if 'input_tumor_aligned' in task.inputs and task.end_time.month == 3:\n",
    "            for key in task.outputs:\n",
    "                # pdb.set_trace()\n",
    "                if re.search(\"vep_vcf\", key) or re.search(\"vep_tbi\", key):\n",
    "                    try:\n",
    "                        out_file_obj = api.files.get(task.outputs[key].id)\n",
    "                    except Exception as e:\n",
    "                        print (e)\n",
    "                        print (\"Error getting file output for \" + task.name + \" \" + task.id + \" skipping!\")\n",
    "                        break\n",
    "                    if out_file_obj.tags == []:\n",
    "                        out_file_obj.tags = [\"PASS\", \"TCGA\", \"SOMATIC\"]\n",
    "                        out_file_obj.save()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Got an error processing \" + task.name + \" \" + task.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"cavatica/openpbta-tcga\"\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "i = 1\n",
    "n = 100\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(parse_march_tag, task): task for task in tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks processed\\n')\n",
    "        i += 1\n",
    "# for task in tasks:\n",
    "#     parse_march_tag(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add metadata and tag NEW outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_to_outputs(task):\n",
    "    try:\n",
    "        if in_key in task.inputs and re.search(phrase, task.name):\n",
    "            metadata = {}\n",
    "            if tag_flag:\n",
    "                tags = task.inputs[\"tool_name\"].split(\"_\")\n",
    "                tags.append(\"NORM\")\n",
    "            for key in task.inputs[in_key].metadata:\n",
    "                metadata[key] = task.inputs[in_key].metadata[key]\n",
    "            # outputs for these tasks are file arrays\n",
    "            for key in task.outputs:\n",
    "                if isinstance(task.outputs[key], list):\n",
    "                    for out_file in task.outputs[key]:\n",
    "                        try:\n",
    "                            out_file_obj = api.files.get(out_file.id)\n",
    "                        except Exception as e:\n",
    "                            print (e)\n",
    "                            print (\"Error getting file output for \" + task.name + \" \" + task.id + \" skipping!\")\n",
    "                            break\n",
    "                        if out_file_obj.metadata is None or len(out_file_obj.metadata) == 0:\n",
    "                            out_file_obj.metadata = metadata\n",
    "                            if tag_flag:\n",
    "                                out_file_obj.tags = tags\n",
    "                            out_file_obj.save()\n",
    "                else:\n",
    "                    try:\n",
    "                        out_file_obj = api.files.get(task.outputs[key].id)\n",
    "                        if out_file_obj.metadata is None or len(out_file_obj.metadata) == 0:\n",
    "                            out_file_obj.metadata = metadata\n",
    "                            if tag_flag:\n",
    "                                out_file_obj.tags = tags\n",
    "                            out_file_obj.save()\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    \n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Got an error processing \" + task.name + \" \" + task.id)\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"kfdrc-harmonization/sd-bhjxbdqk-ad-hoc-caller-rerun\"\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "i = 1\n",
    "n = 100\n",
    "phrase = \"PATCH VCF2MAF RPT\"\n",
    "in_key = \"strelka2_protected_vcf\"\n",
    "tag_flag = False\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(add_meta_to_outputs, task): task for task in tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks processed\\n')\n",
    "        if result.result() is not None:\n",
    "            i += 1\n",
    "\n",
    "# task = api.tasks.get(\"13f0bfe4-fd53-402f-9cc2-30d1caa6d1cd\")\n",
    "# add_meta_to_outputs(task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-run MAF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vcf2maf_refs(api):\n",
    "    ref_dict = {}\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=[\"Homo_sapiens_assembly38.fasta\"])[0]\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_patch_task(t_id):\n",
    "    try:\n",
    "        input_dict = {}\n",
    "        for key in ref_dict:\n",
    "            input_dict[key] = ref_dict[key]\n",
    "        tumor_id = in_dict[t_id]['tumor_id']\n",
    "        normal_id = in_dict[t_id]['normal_id']\n",
    "        for key in in_dict[t_id]:\n",
    "            input_dict[key] = in_dict[t_id][key]\n",
    "        task_name = \"PATCH VCF2MAF RPT: \" + short_name + \" \" + tumor_id + \" \" + normal_id\n",
    "        task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=input_dict, run=False)\n",
    "        task.inputs['output_basename'] = task.id\n",
    "        task.save()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"had a problem drafting a task for \" + old_task_id)\n",
    "        exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_manifest(entry):\n",
    "    # Compress old inputs into single entry\n",
    "    try:\n",
    "        info = entry.rstrip('\\n').split(',')\n",
    "        tumor_id = info[t_idx]\n",
    "        if tumor_id not in in_dict:\n",
    "            in_dict[tumor_id] = {}\n",
    "            in_dict[tumor_id]['tumor_id'] = info[t_idx]\n",
    "            in_dict[tumor_id]['normal_id'] = info[n_idx]\n",
    "        tool_name = ''\n",
    "        # use keys to search file name for key word, then standardize tool name\n",
    "        for key in tool_name_dict:\n",
    "            if re.search(key, info[1]):\n",
    "                tool_name = key\n",
    "                break\n",
    "\n",
    "        if re.search('public', info[1]):\n",
    "            in_dict[tumor_id][tool_name + '_public_vcf'] = api.files.get(info[0])\n",
    "        else:\n",
    "            in_dict[tumor_id][tool_name + '_protected_vcf'] = api.files.get(info[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print (\"Could not process \" + entry)\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out project, vcf_input and short_name vars not used\n",
    "\n",
    "pnoc_project = \"kfdrc-harmonization/sd-8y99qzjj-ad-hoc-caller-rerun\"\n",
    "pnoc_vcf_input = '/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/VCF2MAF_RPT/pnoc_vcf-manifest.csv'\n",
    "pnoc_short_name = \"PNOC\"\n",
    "\n",
    "# cbtn_project = \"kfdrc-harmonization/sd-bhjxbdqk-ad-hoc-caller-rerun\"\n",
    "# cbtn_vcf_input = '/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/VCF2MAF_RPT/cbtn_vcf-manifest.csv'\n",
    "# cbtn_short_name = \"CBTN\"\n",
    "\n",
    "pbta_app = \"/pbta-vcf2maf-patch\"\n",
    "\n",
    "# tcga_project = \"kfdrc-harmonization/openpbta-tcga-ad-hoc-caller-rerun\"\n",
    "# tcga_vcf_input = '/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/VCF2MAF_RPT/tcga_vcfs-manifest.csv'\n",
    "# tcga_short_name = \"TCGA\"\n",
    "# tcga_app = \"/tcga-vcf2maf-patch\"\n",
    "\n",
    "# set these vars to the ones uncommented and to be used in the process\n",
    "project = pnoc_project\n",
    "vcf_input = open(pnoc_vcf_input)\n",
    "short_name = pnoc_short_name\n",
    "\n",
    "app_name = project + pbta_app\n",
    "tool_name_dict = {'strelka2': 'strelka2_somatic', 'mutect2': 'mutect2_somatic',\n",
    "                  'lancet': 'lancet_somatic', 'vardict': 'vardict_somatic'}\n",
    "ref_dict = get_vcf2maf_refs(api)\n",
    "head = next(vcf_input)\n",
    "header = head.rstrip('\\n').split(',')\n",
    "# get tumor and normal ID positions - commented because TCGA is slightly different\n",
    "t_idx = header.index('Kids First Biospecimen ID Tumor')\n",
    "n_idx = header.index('Kids First Biospecimen ID Normal')\n",
    "# t_idx = header.index('bam_tumor_id')\n",
    "# n_idx = header.index('bam_normal_id')\n",
    "in_dict = {}\n",
    "i = 1\n",
    "n = 100\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(squash_manifest, entry): entry for entry in vcf_input}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' input files processed\\n')\n",
    "        i += 1\n",
    "\n",
    "# for entry in vcf_input:\n",
    "#     squash_manifest(entry)\n",
    "i=1\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(draft_patch_task, t_id): t_id for t_id in in_dict}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks created\\n')\n",
    "        i += 1\n",
    "\n",
    "# for task_id in in_dict:\n",
    "#     draft_patch_task(task_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all drafted tasks and an input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"kfdrc-harmonization/sd-bhjxbdqk-ad-hoc-caller-rerun\"\n",
    "tasks = api.tasks.query(project=project, status=\"DRAFT\").all()\n",
    "out = open(\"/Users/brownm28/Documents/2021-Apr-20_norm_annot_patch/MAF_RPT/cbtn_drafted_tasks.txt\", \"w\")\n",
    "out.write(\"task id\\ttask_name\\tpub_input_name\\tpub_input_id\\toutput_basename\\n\")\n",
    "for task in tasks:\n",
    "    if re.search(\"VCF2MAF\", task.name):\n",
    "        bname = task.inputs['output_basename']\n",
    "        if bname is None:\n",
    "            bname = \"MISSING!!!\"\n",
    "        out.write(\"\\t\".join([task.id, task.name, task.inputs['public_vcf'].name, task.inputs['public_vcf'].id, bname]) + \"\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run-a-paloooza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runzilla(task):\n",
    "    try:\n",
    "        task.run()\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Could not run task \" + task.name + \" \" + task.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_project = \"kfdrc-harmonization/openpbta-tcga-ad-hoc-caller-rerun\"\n",
    "cbtn_project = \"kfdrc-harmonization/sd-bhjxbdqk-ad-hoc-caller-rerun\"\n",
    "pnoc_project = \"kfdrc-harmonization/sd-8y99qzjj-ad-hoc-caller-rerun\"\n",
    "\n",
    "project = pnoc_project\n",
    "draft_tasks = api.tasks.query(project=project, status=\"DRAFT\").all()\n",
    "i = 1\n",
    "n = 100\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    results = {executor.submit(runzilla, task): task for task in draft_tasks}\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        if i % n == 0:\n",
    "            sys.stderr.write(str(i) + ' tasks started\\n')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DELME SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'id'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = api.tasks.get(\"20f6e0f5-2096-4fec-8bbe-99c9cf75eb64\")\n",
    "phrase = \"BS_K2G05P1M\"\n",
    "tag_flag = False\n",
    "in_key = \"input_tumor_aligned\"\n",
    "add_meta_to_outputs(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
